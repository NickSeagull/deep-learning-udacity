{
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "name": "",
  "signature": "sha256:231c2a43327311246ebd4bc2277c3d8654298a165f36dd953b33fa951295607a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "kR-4eNdK6lYS"
     },
     "source": [
      "Deep Learning\n",
      "=============\n",
      "\n",
      "Assignment 3\n",
      "------------\n",
      "\n",
      "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
      "\n",
      "The goal of this assignment is to explore regularization techniques."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "# These are all the modules we'll be using later. Make sure you can import them\n",
      "# before proceeding further.\n",
      "from __future__ import print_function\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from six.moves import cPickle as pickle\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab": {
       "autoexec": {
        "startup": false,
        "wait_interval": 0
       }
      },
      "colab_type": "code",
      "id": "JLpLa8Jt7Vu4"
     },
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "1HrCK6e17WzV"
     },
     "source": [
      "First reload the data we generated in _notmist.ipynb_."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle_file = 'notMNIST.pickle'\n",
      "\n",
      "with open(pickle_file, 'rb') as f:\n",
      "  save = pickle.load(f)\n",
      "  train_dataset = save['train_dataset']\n",
      "  train_labels = save['train_labels']\n",
      "  valid_dataset = save['valid_dataset']\n",
      "  valid_labels = save['valid_labels']\n",
      "  test_dataset = save['test_dataset']\n",
      "  test_labels = save['test_labels']\n",
      "  del save  # hint to help gc free up memory\n",
      "  print('Training set', train_dataset.shape, train_labels.shape)\n",
      "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
      "  print('Test set', test_dataset.shape, test_labels.shape)"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab": {
       "autoexec": {
        "startup": false,
        "wait_interval": 0
       },
       "output_extras": [
        {
         "item_id": 1
        }
       ]
      },
      "colab_type": "code",
      "executionInfo": {
       "elapsed": 11777,
       "status": "ok",
       "timestamp": 1449849322348,
       "user": {
        "color": "",
        "displayName": "",
        "isAnonymous": false,
        "isMe": true,
        "permissionId": "",
        "photoUrl": "",
        "sessionId": "0",
        "userId": ""
       },
       "user_tz": 480
      },
      "id": "y3-cj1bpmuxc",
      "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set (200000, 28, 28) (200000,)\n",
        "Validation set (10000, 28, 28) (10000,)\n",
        "Test set (10000, 28, 28) (10000,)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "L7aHrm6nGDMB"
     },
     "source": [
      "Reformat into a shape that's more adapted to the models we're going to train:\n",
      "- data as a flat matrix,\n",
      "- labels as float 1-hot encodings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "image_size = 28\n",
      "num_labels = 10\n",
      "\n",
      "def reformat(dataset, labels):\n",
      "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
      "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
      "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
      "  return dataset, labels\n",
      "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
      "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
      "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
      "print('Training set', train_dataset.shape, train_labels.shape)\n",
      "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
      "print('Test set', test_dataset.shape, test_labels.shape)"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab": {
       "autoexec": {
        "startup": false,
        "wait_interval": 0
       },
       "output_extras": [
        {
         "item_id": 1
        }
       ]
      },
      "colab_type": "code",
      "executionInfo": {
       "elapsed": 11728,
       "status": "ok",
       "timestamp": 1449849322356,
       "user": {
        "color": "",
        "displayName": "",
        "isAnonymous": false,
        "isMe": true,
        "permissionId": "",
        "photoUrl": "",
        "sessionId": "0",
        "userId": ""
       },
       "user_tz": 480
      },
      "id": "IRSyYiIIGIzS",
      "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set (200000, 784) (200000, 10)\n",
        "Validation set (10000, 784) (10000, 10)\n",
        "Test set (10000, 784) (10000, 10)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def accuracy(predictions, labels):\n",
      "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
      "          / predictions.shape[0])"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab": {
       "autoexec": {
        "startup": false,
        "wait_interval": 0
       }
      },
      "colab_type": "code",
      "id": "RajPLaL_ZW6w"
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "sgLbUAQ1CW-1"
     },
     "source": [
      "---\n",
      "Problem 1\n",
      "---------\n",
      "\n",
      "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hidden_nodes = 1024\n",
      "batch_size = 128\n",
      "\n",
      "def propagate_layer_with(dataset, input_weights, input_biases, output_weights, output_biases):\n",
      "    hidden_connection = tf.nn.relu(tf.matmul(dataset, input_weights)+input_biases)\n",
      "    return tf.matmul(hidden_connection, output_weights) + output_biases\n",
      "\n",
      "graph = tf.Graph()\n",
      "with graph.as_default():\n",
      "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
      "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
      "    tf_valid_dataset = tf.constant(valid_dataset)\n",
      "    tf_test_dataset = tf.constant(test_dataset)\n",
      "    \n",
      "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
      "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
      "    \n",
      "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
      "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
      "    \n",
      "    logits = propagate_layer_with(tf_train_dataset,\n",
      "                                  input_weights=weights1,\n",
      "                                  input_biases=biases1,\n",
      "                                  output_weights=weights2,\n",
      "                                  output_biases=biases2)\n",
      "    \n",
      "    C = 5e-4\n",
      "    R = tf.nn.l2_loss(weights1)+tf.nn.l2_loss(weights2)+tf.nn.l2_loss(biases1)+tf.nn.l2_loss(biases2)\n",
      "    \n",
      "    loss = tf.reduce_mean(\n",
      "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)+C*R)\n",
      "        \n",
      "    # Optimizer.\n",
      "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
      "  \n",
      "    # Predictions for the training, validation, and test data.\n",
      "    train_prediction = tf.nn.softmax(logits)\n",
      "    valid_prediction = tf.nn.softmax(propagate_layer_with(tf_valid_dataset, weights1, biases1, weights2, biases2))\n",
      "    test_prediction = tf.nn.softmax(propagate_layer_with(tf_test_dataset, weights1, biases1, weights2, biases2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 3001\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "    tf.initialize_all_variables().run()\n",
      "    print(\"Initialized\")\n",
      "    for step in xrange(num_steps):\n",
      "        # Pick an offset within the training data, which has been randomized.\n",
      "        # Note: we could use better randomization across epochs.\n",
      "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
      "        # Generate a minibatch.\n",
      "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
      "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
      "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
      "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
      "        # and the value is the numpy array to feed to it.\n",
      "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
      "        _, l, predictions = session.run(\n",
      "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
      "        if (step % 500 == 0):\n",
      "            print(\"Minibatch loss at step\", step, \":\", l)\n",
      "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
      "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
      "                valid_prediction.eval(), valid_labels))\n",
      "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "na8xX2yHZzNF"
     },
     "source": [
      "---\n",
      "Problem 2\n",
      "---------\n",
      "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 3001\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "    tf.initialize_all_variables().run()\n",
      "    print(\"Initialized\")\n",
      "    for step in xrange(num_steps):\n",
      "        # Pick an offset within the training data, which has been randomized.\n",
      "        # Note: we could use better randomization across epochs.\n",
      "        offset = (0 * batch_size) % (train_labels.shape[0] - batch_size)\n",
      "        # Generate a minibatch.\n",
      "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
      "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
      "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
      "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
      "        # and the value is the numpy array to feed to it.\n",
      "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
      "        _, l, predictions = session.run(\n",
      "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
      "        if (step % 500 == 0):\n",
      "            print(\"Minibatch loss at step\", step, \":\", l)\n",
      "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
      "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
      "                valid_prediction.eval(), valid_labels))\n",
      "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "ww3SCBUdlkRc"
     },
     "source": [
      "---\n",
      "Problem 3\n",
      "---------\n",
      "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
      "\n",
      "What happens to our extreme overfitting case?\n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hidden_nodes = 1024\n",
      "batch_size = 128\n",
      "dropout_probability = 0.7\n",
      "\n",
      "def propagate_layer_with(dataset, input_weights, input_biases, output_weights, output_biases):\n",
      "    hidden_connection = tf.nn.relu(tf.matmul(dataset, input_weights)+input_biases)\n",
      "    return tf.matmul(tf.nn.dropout(hidden_connection, dropout_probability), output_weights) + output_biases\n",
      "\n",
      "graph = tf.Graph()\n",
      "with graph.as_default():\n",
      "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
      "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
      "    tf_valid_dataset = tf.constant(valid_dataset)\n",
      "    tf_test_dataset = tf.constant(test_dataset)\n",
      "    \n",
      "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
      "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
      "    \n",
      "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
      "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
      "    \n",
      "    logits = propagate_layer_with(tf_train_dataset,\n",
      "                                  input_weights=weights1,\n",
      "                                  input_biases=biases1,\n",
      "                                  output_weights=weights2,\n",
      "                                  output_biases=biases2)\n",
      "    \n",
      "    C = 5e-4\n",
      "    R = tf.nn.l2_loss(weights1)+tf.nn.l2_loss(weights2)+tf.nn.l2_loss(biases1)+tf.nn.l2_loss(biases2)\n",
      "    \n",
      "    loss = tf.reduce_mean(\n",
      "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)+C*R)\n",
      "        \n",
      "    # Optimizer.\n",
      "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
      "  \n",
      "    # Predictions for the training, validation, and test data.\n",
      "    train_prediction = tf.nn.softmax(logits)\n",
      "    valid_prediction = tf.nn.softmax(propagate_layer_with(tf_valid_dataset, weights1, biases1, weights2, biases2))\n",
      "    test_prediction = tf.nn.softmax(propagate_layer_with(tf_test_dataset, weights1, biases1, weights2, biases2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 3001\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "    tf.initialize_all_variables().run()\n",
      "    print(\"Initialized\")\n",
      "    for step in xrange(num_steps):\n",
      "        # Pick an offset within the training data, which has been randomized.\n",
      "        # Note: we could use better randomization across epochs.\n",
      "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
      "        # Generate a minibatch.\n",
      "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
      "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
      "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
      "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
      "        # and the value is the numpy array to feed to it.\n",
      "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
      "        _, l, predictions = session.run(\n",
      "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
      "        if (step % 500 == 0):\n",
      "            print(\"Minibatch loss at step\", step, \":\", l)\n",
      "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
      "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
      "                valid_prediction.eval(), valid_labels))\n",
      "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "-b1hTz3VWZjw"
     },
     "source": [
      "---\n",
      "Problem 4\n",
      "---------\n",
      "\n",
      "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
      "\n",
      "One avenue you can explore is to add multiple layers.\n",
      "\n",
      "Another one is to use learning rate decay:\n",
      "\n",
      "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
      "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
      "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
      " \n",
      " ---\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input_nodes          = image_size * image_size\n",
      "h1_nodes             = 1024\n",
      "h2_nodes             = 300\n",
      "batch_size           = 128\n",
      "dropout_probability  = 0.3\n",
      "learning_rate        = 0.05\n",
      "\n",
      "def propagate_layer_with(inp, input_weights, input_biases, output_weights, output_biases):\n",
      "    hidden_connection = tf.nn.relu(tf.matmul(inp, input_weights)+input_biases)\n",
      "    return tf.matmul(tf.nn.dropout(hidden_connection, 1 - dropout_probability), output_weights) + output_biases\n",
      "\n",
      "graph = tf.Graph()\n",
      "with graph.as_default():\n",
      "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
      "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
      "\n",
      "    weights = {\n",
      "        'h1': tf.Variable(tf.truncated_normal([input_nodes, h1_nodes])),\n",
      "        'h2': tf.Variable(tf.truncated_normal([h1_nodes, h2_nodes])),\n",
      "        'out': tf.Variable(tf.truncated_normal([h2_nodes, num_labels]))\n",
      "    }\n",
      "    \n",
      "    biases = {\n",
      "        'b1': tf.Variable(tf.zeros([h1_nodes])),\n",
      "        'b2': tf.Variable(tf.zeros([h2_nodes])),\n",
      "        'out': tf.Variable(tf.zeros([num_labels]))\n",
      "    }\n",
      "\n",
      "    def network(inp, weights, biases):\n",
      "        layer_1 = tf.nn.dropout(tf.nn.relu6(tf.add(tf.matmul(inp, weights['h1']), biases['b1'])), 1 - dropout_probability)\n",
      "        layer_2 = tf.nn.dropout(tf.nn.relu6(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])), 1 - dropout_probability)\n",
      "        out = tf.matmul(layer_2, weights['out']) + biases['out']\n",
      "        return out\n",
      "\n",
      "    \n",
      "    logits = network(tf_train_dataset, weights, biases)\n",
      "    C = 5e-4\n",
      "    R = tf.nn.l2_loss(weights['h1'])+tf.nn.l2_loss(weights['h2'])+tf.nn.l2_loss(biases['b1'])+tf.nn.l2_loss(biases['b2'])\n",
      "\n",
      "    loss = tf.reduce_mean(\n",
      "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)+C*R)\n",
      "\n",
      "    # Optimizer.\n",
      "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
      "    \n",
      "    \n",
      "    tf_valid_dataset = tf.constant(valid_dataset)\n",
      "    tf_test_dataset = tf.constant(test_dataset)\n",
      "    # Predictions for the training, validation, and test data.\n",
      "    train_prediction = tf.nn.softmax(logits)\n",
      "    valid_prediction = tf.nn.softmax(network(tf_valid_dataset, weights, biases))\n",
      "    test_prediction = tf.nn.softmax(network(tf_test_dataset, weights, biases))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 3001\n",
      "losses = []\n",
      "minibatch_accuracies = []\n",
      "validation_accuracies = []\n",
      "steps = []\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "    tf.initialize_all_variables().run()\n",
      "    print(\"Initialized\")\n",
      "    for step in xrange(num_steps):\n",
      "\n",
      "\n",
      "        # Pick an offset within the training data, which has been randomized.\n",
      "        # Note: we could use better randomization across epochs.\n",
      "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
      "        # Generate a minibatch.\n",
      "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
      "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
      "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
      "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
      "        # and the value is the numpy array to feed to it.\n",
      "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
      "        _, l, predictions = session.run(\n",
      "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
      "        losses.append(l)\n",
      "        if(step % 100 == 0):\n",
      "            steps.append(step)\n",
      "            minibatch_accuracy = accuracy(predictions, batch_labels)\n",
      "            minibatch_accuracies.append(minibatch_accuracy)\n",
      "            validation_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
      "            validation_accuracies.append(validation_accuracy)\n",
      "        if (step % 500 == 0):\n",
      "            print(\"Minibatch loss at step\", step, \":\", l)\n",
      "            print(\"Minibatch accuracy: %.1f%%\" % minibatch_accuracy)\n",
      "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
      "                valid_prediction.eval(), valid_labels))\n",
      "    print(\"Test accuracy: %.1f%%\" % validation_accuracy)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "Minibatch loss at step"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 : 347.997\n",
        "Minibatch accuracy: 9.4%\n",
        "Validation accuracy: 9.6%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500 : 221.809\n",
        "Minibatch accuracy: 59.4%\n",
        "Validation accuracy: 61.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000 : 211.003\n",
        "Minibatch accuracy: 60.9%\n",
        "Validation accuracy: 64.2%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1500 : 203.287\n",
        "Minibatch accuracy: 62.5%\n",
        "Validation accuracy: 61.5%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000 : 196.872\n",
        "Minibatch accuracy: 59.4%\n",
        "Validation accuracy: 59.3%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2500 : 192.488\n",
        "Minibatch accuracy: 60.2%\n",
        "Validation accuracy: 60.4%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Minibatch loss at step"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000 : 187.13\n",
        "Minibatch accuracy: 68.8%\n",
        "Validation accuracy: 65.0%"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Test accuracy: 71.2%\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(steps, minibatch_accuracies, \"b\", steps, validation_accuracies, \"g\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f5fa7eb5550>"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}